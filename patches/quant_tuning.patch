diff --git a/quant_tuning/README.md b/quant_tuning/README.md
new file mode 100644
index 0000000..55563a7
--- /dev/null
+++ b/quant_tuning/README.md
@@ -0,0 +1,27 @@
+<!-- Auto-generated by Codex agent on 2025-12-01T00:27:00Z -->
+# Quantization Tuning Plan
+
+- Target machine: >=32GB RAM, NVMe scratch disk recommended for fast swap; run on a dedicated host (no local CI).
+- Variants to test: `q4_0`, `q4_K_M`, `q4_KV`, `q8_0`, `q8_0-kv-hybrid` (skip automatically if the file is missing).
+- Metrics to collect per run: `p50`, `p95`, `peak_rss`, `load_time_ms` (command duration), `obedience_pass_rate` (if available in result JSON), token throughput (if present), plus metadata (`variant`, `batch_size`, `warmups`, `runs`, `timestamp`).
+- Run matrix: warmups = 3, runs = 20, batch sizes = {1, 4, 8}. Variants looped independently.
+- Output format: JSON arrays under `quant_tuning/results/<variant>_YYYYMMDD.json` where each element is one run (variant + batch size).
+
+## How to execute (remote 32GB+ host)
+1) Copy models and repo to the target machine and activate the desired Python environment.
+2) Run the bench loop (skips missing model files):
+```bash
+bash quant_tuning/run_quant_bench.sh
+```
+   - Override variants or batch sizes: `VARIANTS="q4_0 q8_0" BATCH_SIZES="1 4" bash quant_tuning/run_quant_bench.sh`
+   - Dry-run of the loop logic (no special flag yet) is possible by cancelling after the first echo; the script only calls `bench/benchmark_model.py` which is lightweight compared to quantization.
+3) Aggregate to CSV for quick review:
+```bash
+python quant_tuning/collect_results.py --results-dir quant_tuning/results --output-csv quant_tuning/summary.csv
+```
+4) Move `quant_tuning/results/` and `quant_tuning/summary.csv` off-box for analysis and attach to the PR if needed.
+
+## Notes
+- `bench/benchmark_model.py` produces `bench/bench_results.json`; the helper script copies/annotates that output with variant/batch size and stores it under `quant_tuning/results/`.
+- Token throughput and obedience scores are included only if the underlying bench JSON already provides them (fields default to `null`).
+- Keep warmups/runs consistent across variants to make the CSV comparable; adjust via `WARMUPS`/`RUNS` env vars if you need quicker spot-checks.
diff --git a/quant_tuning/collect_results.py b/quant_tuning/collect_results.py
new file mode 100644
index 0000000..9792d5c
--- /dev/null
+++ b/quant_tuning/collect_results.py
@@ -0,0 +1,96 @@
+# Auto-generated by Codex agent on 2025-12-01T00:27:00Z
+#!/usr/bin/env python3
+"""Aggregate quantization bench outputs into a summary CSV."""
+
+import argparse
+import csv
+import json
+import statistics
+from pathlib import Path
+from typing import Iterable, List
+
+
+def load_runs(results_dir: Path) -> List[dict]:
+    runs: List[dict] = []
+    if not results_dir.exists():
+        return runs
+    for path in results_dir.glob("*.json"):
+        try:
+            data = json.load(path.open("r", encoding="utf-8"))
+        except Exception:
+            continue
+        if isinstance(data, dict):
+            data = [data]
+        for item in data:
+            item = dict(item)
+            item.setdefault("variant", path.stem.split("_")[0])
+            runs.append(item)
+    return runs
+
+
+def _safe_mean(values: Iterable[float | None]) -> float | None:
+    nums = [v for v in values if isinstance(v, (int, float))]
+    if not nums:
+        return None
+    return float(statistics.fmean(nums))
+
+
+def aggregate(runs: List[dict]) -> List[dict]:
+    grouped = {}
+    for run in runs:
+        key = (run.get("variant"), run.get("batch_size"))
+        grouped.setdefault(key, []).append(run)
+
+    summary: List[dict] = []
+    for (variant, batch_size), items in grouped.items():
+        summary.append(
+            {
+                "variant": variant,
+                "batch_size": batch_size,
+                "runs": len(items),
+                "p50_ms_avg": _safe_mean(item.get("p50_ms") for item in items),
+                "p95_ms_avg": _safe_mean(item.get("p95_ms") for item in items),
+                "peak_rss_mb_avg": _safe_mean(item.get("peak_rss_mb") for item in items),
+                "load_time_ms_avg": _safe_mean(item.get("load_time_ms") for item in items),
+                "obedience_pass_rate_avg": _safe_mean(item.get("obedience_pass_rate") for item in items),
+                "tokens_per_sec_avg": _safe_mean(item.get("tokens_per_sec") for item in items),
+            }
+        )
+    return summary
+
+
+def write_csv(summary: List[dict], output_csv: Path) -> None:
+    output_csv.parent.mkdir(parents=True, exist_ok=True)
+    fieldnames = [
+        "variant",
+        "batch_size",
+        "runs",
+        "p50_ms_avg",
+        "p95_ms_avg",
+        "peak_rss_mb_avg",
+        "load_time_ms_avg",
+        "obedience_pass_rate_avg",
+        "tokens_per_sec_avg",
+    ]
+    with output_csv.open("w", newline="", encoding="utf-8") as fh:
+        writer = csv.DictWriter(fh, fieldnames=fieldnames)
+        writer.writeheader()
+        for row in summary:
+            writer.writerow(row)
+
+
+def main() -> int:
+    parser = argparse.ArgumentParser(description=__doc__)
+    parser.add_argument("--results-dir", default="quant_tuning/results", type=Path)
+    parser.add_argument("--output-csv", default="quant_tuning/summary.csv", type=Path)
+    args = parser.parse_args()
+
+    runs = load_runs(args.results_dir)
+    summary = aggregate(runs)
+    write_csv(summary, args.output_csv)
+    print(f"[quant] wrote summary to {args.output_csv} (groups={len(summary)})")
+    return 0
+
+
+if __name__ == "__main__":
+    raise SystemExit(main())
diff --git a/quant_tuning/run_quant_bench.sh b/quant_tuning/run_quant_bench.sh
new file mode 100644
index 0000000..c4f4aa6
--- /dev/null
+++ b/quant_tuning/run_quant_bench.sh
@@ -0,0 +1,94 @@
+#!/usr/bin/env bash
+# Auto-generated by Codex agent on 2025-12-01T00:27:00Z
+set -euo pipefail
+
+REPO_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
+RESULTS_DIR="${REPO_ROOT}/quant_tuning/results"
+mkdir -p "${RESULTS_DIR}"
+
+VARIANTS=${VARIANTS:-"q4_0 q4_K_M q4_KV q8_0 q8_0-kv-hybrid"}
+BATCH_SIZES=${BATCH_SIZES:-"1 4 8"}
+WARMUPS=${WARMUPS:-3}
+RUNS=${RUNS:-20}
+DATE_TAG=${DATE_TAG:-"$(date +%Y%m%d)"}
+MODEL_PREFIX="${REPO_ROOT}/models/gpt-oss-20b"
+
+echo "[quant] repo=${REPO_ROOT}"
+echo "[quant] variants=${VARIANTS}"
+echo "[quant] batch_sizes=${BATCH_SIZES}"
+echo "[quant] warmups=${WARMUPS} runs=${RUNS}"
+
+for variant in ${VARIANTS}; do
+  MODEL_PATH="${MODEL_PREFIX}-${variant}.gguf"
+  if [[ ! -f "${MODEL_PATH}" ]]; then
+    echo "[quant] skip ${variant} (missing ${MODEL_PATH})"
+    continue
+  fi
+
+  OUT_PATH="${RESULTS_DIR}/${variant}_${DATE_TAG}.json"
+  for batch_size in ${BATCH_SIZES}; do
+    echo "[quant] running variant=${variant} batch_size=${batch_size}"
+    START_MS=$(python - <<'PY'
+import time
+print(int(time.time() * 1000))
+PY
+)
+    python "${REPO_ROOT}/bench/benchmark_model.py" --model-path "${MODEL_PATH}" --warmups "${WARMUPS}" --runs "${RUNS}"
+    END_MS=$(python - <<'PY'
+import time
+print(int(time.time() * 1000))
+PY
+)
+    ELAPSED_MS=$((END_MS - START_MS))
+
+    python - <<PY
+import json
+import pathlib
+from datetime import datetime
+
+repo = pathlib.Path("${REPO_ROOT}")
+bench_path = repo / "bench" / "bench_results.json"
+target = pathlib.Path("${OUT_PATH}")
+batch_size = int("${batch_size}")
+variant = "${variant}"
+elapsed_ms = int(${ELAPSED_MS})
+now = datetime.utcnow().isoformat() + "Z"
+
+try:
+    data = json.load(open(bench_path, "r", encoding="utf-8"))
+except FileNotFoundError:
+    raise SystemExit(f"bench results missing at {bench_path}")
+
+entry = {
+    "timestamp": now,
+    "variant": variant,
+    "batch_size": batch_size,
+    "runs": data.get("runs"),
+    "warmups": data.get("warmups"),
+    "model_path": data.get("model_path"),
+    "p50_ms": data.get("latencies_ms", {}).get("p50"),
+    "p95_ms": data.get("latencies_ms", {}).get("p95"),
+    "peak_rss_mb": (data.get("peak_rss") or 0) / (1024 * 1024),
+    "load_time_ms": elapsed_ms,
+    "obedience_pass_rate": data.get("obedience_pass_rate"),
+    "tokens_per_sec": data.get("tokens_per_sec"),
+}
+
+existing = []
+if target.exists():
+    with target.open("r", encoding="utf-8") as fh:
+        try:
+            existing = json.load(fh)
+            if not isinstance(existing, list):
+                existing = [existing]
+        except json.JSONDecodeError:
+            existing = []
+
+existing.append(entry)
+with target.open("w", encoding="utf-8") as fh:
+    json.dump(existing, fh, indent=2)
+
+print(f"[quant] wrote {target} (entries={len(existing)})")
+PY
+  done
+done


