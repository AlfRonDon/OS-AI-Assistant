#!/usr/bin/env bash# WSL setup script to build llama.cpp and quantize GPT-OSS locally.# Run inside WSL (Ubuntu/Debian). Does not download the model automatically.set -euo pipefailsudo apt-get updatesudo apt-get install -y build-essential cmake git python3 python3-venvLLAMA_DIR="${LLAMA_DIR:-$HOME/llama.cpp}"if [ ! -d "$LLAMA_DIR" ]; then  git clone https://github.com/ggerganov/llama.cpp.git "$LLAMA_DIR"else  git -C "$LLAMA_DIR" pull --ff-onlyficd "$LLAMA_DIR"mkdir -p buildcd buildcmake .. -DCMAKE_BUILD_TYPE=Releasecmake --build . --parallelecho "llama.cpp built. To quantize GPT-OSS:"echo "1) Make sure the Windows model path is accessible in WSL, e.g.:"echo "   WIN_MODEL=\"/mnt/c/Users/alfre/OS AI Agent/models/gpt-oss-20b.gguf\""echo "2) Run quantize (choose your preferred type, e.g., Q4_0):"echo "   ./quantize \"$WIN_MODEL\" \"${WIN_MODEL%.gguf}-q.gguf\" q4_0"echo "3) Copy the resulting *_q.gguf back to the Windows models directory if needed."
