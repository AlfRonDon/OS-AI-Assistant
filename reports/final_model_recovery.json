{
    "path_taken":  "D",
    "wsl_script":  "scripts/wsl_model_setup.sh",
    "scan_candidates":  [

                        ],
    "download_attempted":  false,
    "remote_adapter_created":  false,
    "model_file_info":  {
                            "length_bytes":  66,
                            "last_write_time":  "\/Date(1764350135506)\/",
                            "exists":  true
                        },
    "pre_check":  {
                      "model_path":  "models/gpt-oss-20b.gguf",
                      "import_ok":  true,
                      "import_error":  null,
                      "run_error":  "Failed to load model from file: models/gpt-oss-20b.gguf"
                  },
    "post_check":  {
                       "status":  "skipped",
                       "timestamp":  "2025-11-29T20:38:07.4120788+05:30",
                       "reason":  "path D selected (WSL helper); no post-check run"
                   },
    "rebuild_test_summary":  null,
    "recommendations":  [
                            "No GGUF candidates \u003e300MB found; locate a real GPT-OSS model and place it at models/gpt-oss-20b.gguf.",
                            "If you have a URL, set DOWNLOAD_URL=https://... in .env to allow automated download.",
                            "Run scripts/wsl_model_setup.sh inside WSL to build llama.cpp tools and quantize a local model (see script instructions).",
                            "Optionally set USE_REMOTE_MODEL=1 with REMOTE_PROVIDER to use a remote planner until a local model is available.",
                            "Install python deps (psutil, llama-cpp-python) so check_model_load.py can validate models."
                        ]
}
